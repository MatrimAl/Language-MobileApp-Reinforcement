{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d0328f",
   "metadata": {},
   "source": [
    "# ðŸ§  DQN Language Learning - Model Training Notebook\n",
    "\n",
    "Bu notebook'ta DQN modelini offline olarak eÄŸiteceÄŸiz ve sonuÃ§larÄ± analiz edeceÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523158eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../backend')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dqn_agent import DQNAgent\n",
    "from rl_environment import LanguageLearningEnv\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b9241",
   "metadata": {},
   "source": [
    "## 1. Sample Word Pool OluÅŸturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample word pool (50 kelime)\n",
    "word_pool = [\n",
    "    {\"id\": f\"{i}\", \"word\": f\"word_{i}\", \"translation\": f\"kelime_{i}\", \n",
    "     \"difficulty\": (i % 5) + 1, \"language\": \"en\"}\n",
    "    for i in range(1, 51)\n",
    "]\n",
    "\n",
    "# Zorluk daÄŸÄ±lÄ±mÄ±\n",
    "difficulty_dist = pd.Series([w['difficulty'] for w in word_pool]).value_counts().sort_index()\n",
    "print(\"ðŸ“Š Zorluk DaÄŸÄ±lÄ±mÄ±:\")\n",
    "print(difficulty_dist)\n",
    "\n",
    "# GÃ¶rselleÅŸtirme\n",
    "plt.figure(figsize=(8, 5))\n",
    "difficulty_dist.plot(kind='bar', color='skyblue')\n",
    "plt.title('Word Pool - Difficulty Distribution')\n",
    "plt.xlabel('Difficulty Level')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460512b9",
   "metadata": {},
   "source": [
    "## 2. Environment ve Agent OluÅŸturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \"source\": [\n",
    "    \"# Create environment\\n\",\n",
    "    \"env = LanguageLearningEnv(word_pool=word_pool)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"ðŸ“¦ Environment:\\\")\\n\",\n",
    "    \"print(f\\\"  State Space: {env.observation_space.shape}\\\")\\n\",\n",
    "    \"print(f\\\"  Action Space: {env.action_space.n}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create DQN agent (PyTorch)\\n\",\n",
    "    \"agent = DQNAgent(\\n\",\n",
    "    \"    state_size=12,\\n\",\n",
    "    \"    action_size=5,\\n\",\n",
    "    \"    learning_rate=0.001,\\n\",\n",
    "    \"    epsilon=1.0,\\n\",\n",
    "    \"    epsilon_decay=0.995,\\n\",\n",
    "    \"    batch_size=32\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nðŸ¤– DQN Agent (PyTorch):\\\")\\n\",\n",
    "    \"print(f\\\"  Device: {agent.device}\\\")\\n\",\n",
    "    \"print(f\\\"  Initial Epsilon: {agent.epsilon}\\\")\\n\",\n",
    "    \"print(f\\\"  Learning Rate: {agent.learning_rate}\\\")\\n\",\n",
    "    \"print(f\\\"  Batch Size: {agent.batch_size}\\\")\\n\",\n",
    "    \"print(f\\\"  Model Parameters: {sum(p.numel() for p in agent.model.parameters()):,}\\\")\"\n",
    "   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96839c2f",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ec312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56deff47",
   "metadata": {},
   "source": [
    "## 4. Training Loop (100 Episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96668957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPISODES = 100\n",
    "UPDATE_TARGET_FREQ = 10\n",
    "\n",
    "# Metrics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "epsilon_values = []\n",
    "losses = []\n",
    "\n",
    "print(\"ðŸš€ Starting training...\\n\")\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Select action\n",
    "        action = agent.act(state, training=True)\n",
    "        \n",
    "        # Take step\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Remember\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn\n",
    "        loss = agent.replay()\n",
    "        if loss > 0:\n",
    "            losses.append(loss)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "    \n",
    "    # Update target network\n",
    "    if (episode + 1) % UPDATE_TARGET_FREQ == 0:\n",
    "        agent.update_target_model()\n",
    "    \n",
    "    # Record metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    epsilon_values.append(agent.epsilon)\n",
    "    \n",
    "    # Print progress\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        print(f\"Episode {episode + 1}/{EPISODES} | \"\n",
    "              f\"Reward: {episode_reward:.2f} | \"\n",
    "              f\"Avg (10): {avg_reward:.2f} | \"\n",
    "              f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144bea1",
   "metadata": {},
   "source": [
    "## 5. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b817de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Episode Rewards\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.6, label='Episode Reward')\n",
    "axes[0, 0].plot(pd.Series(episode_rewards).rolling(10).mean(), \n",
    "                linewidth=2, label='Moving Avg (10)')\n",
    "axes[0, 0].set_title('Episode Rewards', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Epsilon Decay\n",
    "axes[0, 1].plot(epsilon_values, color='orange', linewidth=2)\n",
    "axes[0, 1].set_title('Epsilon Decay (Exploration Rate)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Epsilon')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Episode Lengths\n",
    "axes[1, 0].plot(episode_lengths, color='green', alpha=0.6)\n",
    "axes[1, 0].set_title('Episode Lengths', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Steps')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training Loss\n",
    "if losses:\n",
    "    axes[1, 1].plot(losses, color='red', alpha=0.4)\n",
    "    axes[1, 1].plot(pd.Series(losses).rolling(50).mean(), \n",
    "                    linewidth=2, label='Moving Avg (50)')\n",
    "    axes[1, 1].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Training Step')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11594d37",
   "metadata": {},
   "source": [
    "## 6. Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b750ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "stats = {\n",
    "    'Total Episodes': EPISODES,\n",
    "    'Avg Reward (All)': np.mean(episode_rewards),\n",
    "    'Avg Reward (Last 20)': np.mean(episode_rewards[-20:]),\n",
    "    'Max Reward': np.max(episode_rewards),\n",
    "    'Min Reward': np.min(episode_rewards),\n",
    "    'Final Epsilon': epsilon_values[-1],\n",
    "    'Avg Episode Length': np.mean(episode_lengths)\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“Š Training Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:.<30} {value:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aeacd8",
   "metadata": {},
   "source": [
    "## 7. Test Agent (Inference Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cfd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with epsilon=0 (pure exploitation)\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "test_episodes = 5\n",
    "test_rewards = []\n",
    "\n",
    "print(\"ðŸ§ª Testing agent (greedy policy)...\\n\")\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    print(f\"Test Episode {episode + 1}:\")\n",
    "    \n",
    "    while not done and step < 10:  # Max 10 steps for demo\n",
    "        action = agent.act(state, training=False)\n",
    "        q_values = agent.get_q_values(state)\n",
    "        \n",
    "        print(f\"  Step {step + 1}: Action={action} (Difficulty Level), Q-values={q_values}\")\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        print(f\"    â†’ Reward: {reward:.2f}, Word: {info['word']}, Correct: {info['is_correct']}\")\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        step += 1\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "    print(f\"  Total Reward: {episode_reward:.2f}\\n\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Test Results:\")\n",
    "print(f\"  Avg Test Reward: {np.mean(test_rewards):.2f}\")\n",
    "print(f\"  Max Test Reward: {np.max(test_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200c759",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0be4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "save_path = \"../backend/models/dqn_notebook\"\n",
    "agent.save(save_path)\n",
    "\n",
    "print(f\"âœ… Model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fb38f",
   "metadata": {},
   "source": [
    "## 9. Action Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze action selection\n",
    "action_counts = [0] * 5\n",
    "\n",
    "for _ in range(100):\n",
    "    state, _ = env.reset()\n",
    "    action = agent.act(state, training=False)\n",
    "    action_counts[action] += 1\n",
    "\n",
    "# Visualization\n",
    "difficulty_names = ['Beginner', 'Elementary', 'Intermediate', 'Advanced', 'Expert']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(difficulty_names, action_counts, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6'])\n",
    "plt.title('Action Distribution (100 Random States)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Difficulty Level')\n",
    "plt.ylabel('Selection Count')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Action Distribution:\")\n",
    "for name, count in zip(difficulty_names, action_counts):\n",
    "    print(f\"  {name:.<20} {count:>3} ({count/100*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98136319",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Notebook Complete!\n",
    "\n",
    "Bu notebook'ta:\n",
    "- âœ… DQN agent'i baÅŸarÄ±yla eÄŸittik\n",
    "- âœ… Training metriklerini gÃ¶rselleÅŸtirdik\n",
    "- âœ… Model'i test ettik\n",
    "- âœ… Action distribution analizi yaptÄ±k\n",
    "\n",
    "**Sonraki AdÄ±mlar:**\n",
    "1. Modeli backend API'ye yÃ¼kle\n",
    "2. Mobil uygulamada test et\n",
    "3. Dashboard'da visualize et"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
