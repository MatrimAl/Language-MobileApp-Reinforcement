# A2 Seviye SeÃ§im Problemi - DetaylÄ± Analiz

## ğŸ¯ Problem Ã–zeti

**Durum:** KullanÄ±cÄ±nÄ±n hedef seviyesi B2 olmasÄ±na raÄŸmen, RL agent 20-30 episode boyunca sÃ¼rekli A2 seviyesi seÃ§ti.

**Sistem:** DQN (Deep Q-Network) tabanlÄ± TÃ¼rkÃ§e-Ä°ngilizce kelime Ã¶ÄŸrenme uygulamasÄ±

---

## ğŸ“Š Sistem DetaylarÄ±

### Agent YapÄ±sÄ±
- **Model:** Deep Q-Network (DQN)
- **Mimari:** 13 input â†’ 128 hidden â†’ 128 hidden â†’ 5 output
- **State Boyutu:** 13 Ã¶zellik
- **Action SayÄ±sÄ±:** 5 (A1, A2, B1, B2, C1)
- **Epsilon:** 0.1 (10% exploration, 90% exploitation)
- **Replay Buffer:** 50,000 transition
- **Batch Size:** 64
- **Learning Rate:** 0.001
- **Gamma:** 0.99
- **Target Network Update:** Her 2000 step'te hard update

### State VektÃ¶rÃ¼ (13 boyutlu)
```python
[
    A1_accuracy,      # 0: A1 seviyesi baÅŸarÄ± oranÄ± (Laplace smoothing)
    A2_accuracy,      # 1: A2 seviyesi baÅŸarÄ± oranÄ±
    B1_accuracy,      # 2: B1 seviyesi baÅŸarÄ± oranÄ±
    B2_accuracy,      # 3: B2 seviyesi baÅŸarÄ± oranÄ±
    C1_accuracy,      # 4: C1 seviyesi baÅŸarÄ± oranÄ±
    moving_accuracy,  # 5: Son 50 denemenin ortalamasÄ±
    response_time,    # 6: Normalize edilmiÅŸ cevap sÃ¼resi (0-1)
    due_ratio,        # 7: GecikmiÅŸ kelime oranÄ±
    target_A1,        # 8: Hedef seviye one-hot (0 for B2)
    target_A2,        # 9: (0 for B2)
    target_B1,        # 10: (0 for B2)
    target_B2,        # 11: (1 for B2) â† HEDEF
    target_C1         # 12: (0 for B2)
]
```

### Reward Fonksiyonu
```python
r = base_reward + 0.2 * diff_bonus + 0.1 * due_bonus - 0.05 * time_penalty

# base_reward: 1.0 (doÄŸru), 0.0 (yanlÄ±ÅŸ)
# diff_bonus: Kelime seviyesi ile hedef seviye arasÄ±ndaki uyum bonusu
#   - Optimal (0 fark): 1.0
#   - 1 seviye fark: 0.8
#   - 2 seviye fark: 0.6
# due_bonus: GecikmiÅŸ kelime bonusu
# time_penalty: Cevap sÃ¼resi cezasÄ±
```

**Ã–rnek Ã–dÃ¼ller (Hedef: B2):**
- B2 kelimesi doÄŸru: `1.0 + 0.2*1.0 + 0.1*x - 0.05*y â‰ˆ 1.00`
- A2 kelimesi doÄŸru: `1.0 + 0.2*0.8 + 0.1*x - 0.05*y â‰ˆ 0.98`
- **Fark sadece 0.02!** âš ï¸

---

## ğŸ” GÃ¶zlemlenen DavranÄ±ÅŸ

### Episode DaÄŸÄ±lÄ±mÄ±
```
Episode  1-19: Ã‡oÄŸunlukla B2 seÃ§ildi âœ…
Episode 20   : A2 seÃ§ildi (rastgele exploration)
Episode 21-40: SÃ¼rekli A2 seÃ§ildi! âŒ
```

### Performans Metrikleri
```
Seviye | BaÅŸarÄ± | Deneme SayÄ±sÄ±
-------|--------|---------------
A1     | 73.1%  |  93
A2     | 79.3%  | 198  â† En yÃ¼ksek baÅŸarÄ±!
B1     | 74.6%  | 551
B2     | 75.1%  | 668
C1     | 63.2%  | 155

Son 20 Episode BaÅŸarÄ±: 95.0%
```

### A2 SeÃ§im Paterni
```
Episode 20-40:
[A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, 
 A2, A2, A2, A2, A2, A2, B2]

A2 Ã–dÃ¼lleri: 0.98 (istikrarlÄ± ve yÃ¼ksek)
B2 Ã–dÃ¼lleri: 1.00 (ama bazen 0.0 - deÄŸiÅŸken)
```

---

## â“ Sorular

### Ana Soru
**Hedef seviye B2 olmasÄ±na raÄŸmen agent neden sÃ¼rekli A2 seÃ§iyor?**

### Alt Sorular

1. **Reward TasarÄ±mÄ±:**
   - A2 ile B2 arasÄ±ndaki Ã¶dÃ¼l farkÄ± (0.02) Ã§ok kÃ¼Ã§Ã¼k mÃ¼?
   - Hedef seviyeye uygunluk iÃ§in daha bÃ¼yÃ¼k bir bonus gerekli mi?
   - Diff bonus katsayÄ±sÄ± 0.2 yerine 0.5 veya 1.0 olmalÄ± mÄ±?

2. **State Temsili:**
   - Hedef seviyeyi one-hot encoding olarak ekledik ama agent bunu yeterince kullanÄ±yor mu?
   - State'e baÅŸka Ã¶zellikler eklemeli miyiz?

3. **Exploration/Exploitation:**
   - Epsilon 0.1 Ã§ok mu yÃ¼ksek?
   - Agent yeterince exploitation yapamÄ±yor mu?
   - Epsilon decay kullanmalÄ± mÄ±yÄ±z? (0.1 â†’ 0.01 gibi)

4. **Q-DeÄŸerleri:**
   - DQN'nin Q-deÄŸerleri doÄŸru yakÄ±nsÄ±yor mu?
   - A2'nin Q-deÄŸeri yanlÄ±ÅŸlÄ±kla B2'den yÃ¼ksek mi Ã¶ÄŸrenildi?
   - Q-deÄŸerlerini nasÄ±l inceleyebiliriz?

5. **DavranÄ±ÅŸ Analizi:**
   - Bu geÃ§ici bir exploration phase mi?
   - Yoksa kalÄ±cÄ± bir Ã¶ÄŸrenme hatasÄ± mÄ±?
   - A2'nin yÃ¼ksek baÅŸarÄ± oranÄ± (%79.3) ve istikrarlÄ± Ã¶dÃ¼lleri agent'Ä± "kandÄ±rÄ±yor" mu?

6. **Ã‡Ã¶zÃ¼m Stratejileri:**
   - Reward shaping yapmalÄ± mÄ±yÄ±z?
   - Hedef seviyeden uzaklaÅŸma iÃ§in penalty eklemeli miyiz?
   - Experience replay'deki Ã¶nceliklendirme deÄŸiÅŸtirilmeli mi?

---

## ğŸ¯ Beklenen DavranÄ±ÅŸ

**Ä°deal:**
- Agent Ã§oÄŸunlukla B2 (hedef seviye) seÃ§meli
- Bazen B1 veya C1 seÃ§ebilir (yakÄ±n seviyeler)
- %10 exploration ile ara sÄ±ra A2 veya A1 seÃ§meli

**GerÃ§ekleÅŸen:**
- 20-30 episode boyunca sÃ¼rekli A2 seÃ§ildi
- Hedef seviye B2 olmasÄ±na raÄŸmen agent "takÄ±ldÄ±"

---

## ğŸ’¡ Hipotezler

### Hipotez 1: Reward FarkÄ± Ã‡ok KÃ¼Ã§Ã¼k
A2 kelimeleri Ã§ok kolay olduÄŸu iÃ§in sÃ¼rekli doÄŸru cevaplanÄ±yor â†’ 0.98 Ã¶dÃ¼l her seferinde garantili. B2 kelimeleri daha zor, bazen yanlÄ±ÅŸ â†’ 0.00 Ã¶dÃ¼l riski var. Agent "gÃ¼venli" olanÄ± (A2) tercih ediyor.

**Ã‡Ã¶zÃ¼m:** Diff bonus katsayÄ±sÄ±nÄ± artÄ±r (0.2 â†’ 0.5)

### Hipotez 2: State Yeterince Ä°yi DeÄŸil
One-hot encoding hedef seviyeyi gÃ¶steriyor ama agent DQN aÄŸÄ±rlÄ±klarÄ±nda bunu yeterince kullanmÄ±yor.

**Ã‡Ã¶zÃ¼m:** Hedef seviye ile action arasÄ±ndaki farkÄ± explicit olarak state'e ekle

### Hipotez 3: Exploration Fazla
%10 exploration ile random A2 seÃ§ildi, sonra Q-deÄŸerleri bu yÃ¶nde gÃ¼ncellendi ve agent A2'de "takÄ±ldÄ±".

**Ã‡Ã¶zÃ¼m:** Epsilon'u 0.05'e dÃ¼ÅŸÃ¼r veya epsilon decay kullan

---

## ğŸ› ï¸ Ä°stenen YardÄ±m

1. Bu davranÄ±ÅŸÄ±n **kÃ¶k nedenini** bulmak
2. **Ã‡Ã¶zÃ¼m Ã¶nerileri** almak:
   - Reward fonksiyonu nasÄ±l deÄŸiÅŸtirilmeli?
   - State tasarÄ±mÄ± iyileÅŸtirilmeli mi?
   - Hyperparameter'lar (epsilon, learning rate, vb.) ayarlanmalÄ± mÄ±?
3. Benzer problemlerle karÅŸÄ±laÅŸanlarÄ±n **deneyimleri**
4. DQN iÃ§in **best practices** (RL context'inde hedef-driven selection)

---

## ğŸ“ Kod Ã–rnekleri

### Agent Act Fonksiyonu
```python
def act(self, s, eps=0.1):
    self.steps += 1
    import random
    if random.random() < eps:
        return random.randrange(self.n_actions)
    with torch.no_grad():
        q = self.q(torch.tensor(s, dtype=torch.float32).unsqueeze(0))
        return int(q.argmax(dim=1).item())
```

### Reward Hesaplama
```python
def compute_reward(correct, word, user, response_ms):
    base = 1.0 if correct else 0.0
    
    # Kelime seviyesi ile hedef seviye arasÄ±ndaki fark
    word_level_idx = LEVELS.index(word.level)
    target_level_idx = LEVELS.index(user.target_level)
    diff = abs(word_level_idx - target_level_idx)
    diff_bonus = max(0, 1 - diff * 0.2)
    
    # Due bonus
    due_bonus = calculate_due_bonus(...)
    
    # Time penalty
    time_penalty = min(response_ms / 12000.0, 1.0)
    
    r = base + 0.2 * diff_bonus + 0.1 * due_bonus - 0.05 * time_penalty
    return r
```

### State OluÅŸturma
```python
def build_state(db, user):
    # Seviye baÅŸarÄ± oranlarÄ± (Laplace smoothing)
    level_accs = []
    for level in LEVELS:
        stat = get_level_stat(db, user.id, level)
        acc = (stat.correct + 1) / (stat.correct + stat.wrong + 2)
        level_accs.append(acc)
    
    # Hareketli ortalama
    moving_acc = moving_accuracy(db, user.id, k=50)
    
    # Cevap sÃ¼resi
    response_time = normalize_response_time(...)
    
    # Due ratio
    due_ratio = calculate_due_ratio(...)
    
    # Hedef seviye one-hot
    target_idx = LEVELS.index(user.target_level)
    target_one_hot = [1.0 if i == target_idx else 0.0 for i in range(5)]
    
    return level_accs + [moving_acc, response_time, due_ratio] + target_one_hot
```

---

**Not:** Bu bir eÄŸitim projesi olduÄŸu iÃ§in teorik aÃ§Ä±klamalar ve pratik Ã§Ã¶zÃ¼mler bekliyorum. TeÅŸekkÃ¼rler! ğŸ™
